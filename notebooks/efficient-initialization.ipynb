{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31644638-a09f-4e7b-b4a7-2883e6ce10f1",
   "metadata": {},
   "source": [
    "# Efficient Initialization of Large Models\n",
    "\n",
    "* [Blog](https://lightning.ai/pages/community/efficient-initialization-of-large-models/)\n",
    "\n",
    "We will be using `EleutherAI/pythia-1b` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bda0f42-0e47-4140-9e77-f8bcfb780e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1fd0ec9-6b67-4646-a808-56f2f3be3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_gpt import GPT, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af56524f-2cfb-4c67-9af5-4131802394ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_device(model):\n",
    "    return next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f221823c-a468-45c2-88fd-e80021835edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/data/aniket/meta-llama/Llama-2-7b-chat-hf/lit_model.pth\"\n",
    "checkpoint_dir = \"/data/aniket/meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "checkpoints = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79f6979-7eeb-4787-86ca-be6d2904d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load model: 39.23 seconds.\n",
      "This is your old nn.Module: True\n",
      "device cpu\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "model = GPT.from_name(model_name)\n",
    "model.load_state_dict(checkpoints)\n",
    "print(f\"Time to load model: {time.time() - t0:.02f} seconds.\")\n",
    "print(\"This is your old nn.Module:\", isinstance(model, torch.nn.Module))\n",
    "print(\"device\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1688eb8-6b7e-4cd7-bc1d-2ee1169945b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d402e4-81c9-484a-9543-46ca3c7979da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 0.00 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'checkpoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fabric\u001b[38;5;241m.\u001b[39minit_module():\n\u001b[1;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m GPT\u001b[38;5;241m.\u001b[39mfrom_name(model_name)\n\u001b[0;32m----> 7\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoints\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to load model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoints' is not defined"
     ]
    }
   ],
   "source": [
    "fabric = L.Fabric(accelerator=\"gpu\", precision=\"bf16-true\")\n",
    "\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n",
    "t0 = time.time()\n",
    "with fabric.init_module():\n",
    "    model = GPT.from_name(model_name)\n",
    "    model.load_state_dict(checkpoints)\n",
    "print(f\"Time to load model: {time.time() - t0:.02f} seconds.\")\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36727e06-4dc9-4547-9d36-2086afcf07e4",
   "metadata": {},
   "source": [
    "\n",
    "7B\n",
    "\n",
    "32 bits => 4 bytes\n",
    "\n",
    "7 x 10^9 x 32 bits => 28 x 10^9 x bytes\n",
    "\n",
    "28 GB memory\n",
    "\n",
    "---\n",
    "\n",
    "16 bits => 2 bytes\n",
    "\n",
    "7 x 10^9 x 16 bits => 14 x 10^9 x bytes\n",
    "\n",
    "14 GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff4030-0f95-4188-8d8c-d49bef539e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dde81501-3b83-47a1-9f7c-805d066f9769",
   "metadata": {},
   "source": [
    "## Let's play with non Lightning models now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc0732c-355f-4924-ac92-c6cb423f7888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78bc0e902b44bb3bd0a462dfb8c0d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c8e11-dc2d-4a40-98e6-268768d65ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8816f2c1-fd83-4c82-a74c-5e67af6069d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e7a45b93604067a7fedc1f70c184dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 4.42 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                                device_map={\"\": 0},)\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c88e26a-35f9-428e-b394-17a377830611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model_device(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af820dd-5964-4d0c-9e80-d5d2469dcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = L.Fabric(accelerator=\"gpu\", precision=\"bf16-true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ad3457-143c-4bca-82ea-f5fbff2eb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 0.00 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099cf6c851b5453da3ad748efae27bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 13.57 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n",
    "with fabric.init_module():\n",
    "    model_hf = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d23fe2-7059-4956-960a-77897bc70bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(32000, 4096)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (attn): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): LLaMAMLP(\n",
       "          (fc_1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (fc_2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a2ffe17-45a4-4c7e-9b2c-79ef62566017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f06a850-c616-42bc-92f0-20169a0d3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c788f334-ce0f-4a2c-864f-9d2cd5093e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  1, 306, 626, 530, 638, 300]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer_hf(\"I am Aniket\", max_length=512, return_tensors=\"pt\", truncation=True)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41f629a-4cf4-4a47-b912-77082f0bc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(Path(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727d88da-ae20-44e1-b1dd-f20fc197e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 13.48 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a122b3c-706b-43ff-b61b-b513f6e22bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with fabric.device:\n",
    "    encoded = tokenizer.encode([\"What is my name?\"*10]*2)\n",
    "encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3a1246-af15-40a2-95d9-b1c8a9849b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 14.92 GB\n"
     ]
    }
   ],
   "source": [
    "logits = model(encoded, max_seq_length=512)\n",
    "model.reset_cache()\n",
    "print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d9369c-26b4-4205-9634-7299c19e69e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 32000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cfe6cab-fd1e-4009-b867-f085a97f458b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49, 32000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits[..., :-1, :]\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f684193-e15d-4885-aa6b-7501da4a99dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([98, 32000])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits.reshape(-1, logits.size(-1))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6b52b87f-56ac-4c90-8a4d-5e4102205efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits[..., :-1, :], targets[..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab8770-f92a-411a-a0c6-6133b0d609f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe81bfa-3aad-4673-b998-d9ee88dad7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
